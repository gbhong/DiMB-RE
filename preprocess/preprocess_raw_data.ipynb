{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This notebook file is the first step to preprocess BRAT format files and split them into Abstracts-only and Results-only files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import statistics\n",
    "import pandas as pd\n",
    "\n",
    "import ssplit\n",
    "from tokenization_bert import BasicTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n",
      "165\n"
     ]
    }
   ],
   "source": [
    "# Input raw files\n",
    "raw_file_path = \"../data/brat/raw_files\"\n",
    "\n",
    "# Load raw files\n",
    "ann_files = []\n",
    "txt_files = []\n",
    "for file_name in os.listdir(raw_file_path):\n",
    "    if file_name.endswith(\".ann\"):\n",
    "        ann_files.append(file_name)\n",
    "    elif file_name.endswith(\".txt\"):\n",
    "        txt_files.append(file_name)\n",
    "\n",
    "ann_files.sort()\n",
    "txt_files.sort()\n",
    "print(len(ann_files), (len(txt_files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter BRAT annotation errors and Split raw dataset into 1) Title-Abstract and 2) Result part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load offset information for fully annotated text\n",
    "offset_path = \"../data/brat/AnnotationPMC.csv\"\n",
    "offset_dict = {}\n",
    "with open(offset_path, \"r\") as f:\n",
    "    next(f)\n",
    "    for line in f:\n",
    "        line = line.strip().rstrip(\",\")\n",
    "        PMCID, OFFSETS = line.split(\",\", 1)\n",
    "        if len(OFFSETS.split(\",\")) > 1:\n",
    "            ABS_END, RES_OFFSET, RES_END = OFFSETS.split(\",\")\n",
    "        else:\n",
    "            ABS_END = OFFSETS\n",
    "            \n",
    "        offset_dict[PMCID] = {}\n",
    "        offset_dict[PMCID][\"Abs_end\"] = int(ABS_END)\n",
    "        if RES_OFFSET:\n",
    "            offset_dict[PMCID][\"Res_offset\"] = int(RES_OFFSET)\n",
    "            offset_dict[PMCID][\"Res_end\"] = int(RES_END)\n",
    "            \n",
    "        ABS_END = 0\n",
    "        RES_OFFSET = 0\n",
    "        RES_END = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch if you want to include all 15 types of entities\n",
    "processed_file_path = \"../data/brat/processed_files\"\n",
    "# processed_file_path = \"../data/brat/processed_files_full_entities\"\n",
    "\n",
    "# Output files\n",
    "title_abs_path = os.path.join(processed_file_path, \"title_abs\")\n",
    "results_path = os.path.join(processed_file_path, \"results\")\n",
    "\n",
    "os.makedirs(title_abs_path, exist_ok=True)\n",
    "os.makedirs(results_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_TOKENIZER = BasicTokenizer(do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 165/165 [00:01<00:00, 145.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Sentences in 165 Abs corpus: 2181 | Med.: 13\n",
      "# Words in 165 Abs corpus: 68136 | Med.: 411\n",
      "# Sentences in 30 Res corpus: 2129 | Med.: 28.5\n",
      "# Words in 30 Res corpus: 68911 | Med.: 944.5\n",
      "# Tables in 30 Res corpus: 58 | Mean: 1.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create 30 'results-only' txt files\n",
    "\n",
    "num_sents_abs = []\n",
    "num_tokens_abs = []\n",
    "num_sents_res = []\n",
    "num_tokens_res = []\n",
    "num_tables_res = 0\n",
    "\n",
    "for txt in tqdm(txt_files):\n",
    "    doc_key = txt.strip(\".txt\")\n",
    "    \n",
    "    doc_sents_abs = 0\n",
    "    doc_tokens_abs = 0\n",
    "    doc_sents_res = 0\n",
    "    doc_tokens_res = 0\n",
    "    \n",
    "    with open(os.path.join(raw_file_path, txt), encoding=\"utf-8\") as f:\n",
    "        doc = f.read()\n",
    "        \n",
    "    if doc_key.startswith('PMC'):\n",
    "        offsets = offset_dict[doc_key]\n",
    "        if 'Res_offset' in offset_dict[doc_key].keys():\n",
    "            result = doc[offsets[\"Res_offset\"]:offsets[\"Res_end\"]]\n",
    "            \n",
    "            # Count number of words and sentences\n",
    "            sentences = re.split(r\"[\\.!\\?]\\s(?=[A-Z])|\\n+\", result)\n",
    "            for sent in sentences:\n",
    "                \n",
    "                # Detect table format\n",
    "                if '\\t' in sent:\n",
    "                    num_tables_res += 1\n",
    "                else:\n",
    "                    doc_sents_res += 1\n",
    "                    sent += \".\" if sent[-1] != \".\" else sent\n",
    "                    doc_tokens_res += len(BASIC_TOKENIZER.tokenize(sent))\n",
    "\n",
    "            result_fname = doc_key + \"-RESULT.txt\"\n",
    "            with open(os.path.join(results_path, result_fname), \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(result)\n",
    "\n",
    "        result = doc[:offsets[\"Abs_end\"]]\n",
    "\n",
    "        # Count number of words and sentences\n",
    "        sentences = re.split(r\"[\\.!\\?]\\s(?=[A-Z])|\\n+\", result)\n",
    "        for sent in sentences:\n",
    "            doc_sents_abs += 1\n",
    "            sent += \".\" if sent[-1] != \".\" else sent\n",
    "            doc_tokens_abs += len(BASIC_TOKENIZER.tokenize(sent))\n",
    "\n",
    "        result_fname = doc_key + \".txt\"\n",
    "        with open(os.path.join(title_abs_path, result_fname), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(result)\n",
    "            \n",
    "        num_sents_abs.append(doc_sents_abs)\n",
    "        num_tokens_abs.append(doc_tokens_abs)\n",
    "        num_sents_res.append(doc_sents_res)\n",
    "        num_tokens_res.append(doc_tokens_res)\n",
    "            \n",
    "    else:  # just re-write for Non-PMC files            \n",
    "        # Count number of words and sentences\n",
    "        sentences = re.split(r\"[\\.!\\?]\\s(?=[A-Z])|\\n+\", doc)\n",
    "        for sent in sentences:\n",
    "            doc_sents_abs += 1\n",
    "            sent += \".\" if sent[-1] != \".\" else sent\n",
    "            doc_tokens_abs += len(BASIC_TOKENIZER.tokenize(sent))\n",
    "            \n",
    "        result_fname = doc_key + \".txt\"\n",
    "        with open(os.path.join(title_abs_path, result_fname), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(doc)\n",
    "            \n",
    "        num_sents_abs.append(doc_sents_abs)\n",
    "        num_tokens_abs.append(doc_tokens_abs)\n",
    "            \n",
    "print(f\"# Sentences in 165 Abs corpus: {sum(num_sents_abs)} | Med.: {statistics.median(num_sents_abs)}\")\n",
    "print(f\"# Words in 165 Abs corpus: {sum(num_tokens_abs)} | Med.: {statistics.median(num_tokens_abs)}\")\n",
    "print(f\"# Sentences in 30 Res corpus: {sum(num_sents_res)} | Med.: {statistics.median(num_sents_res)}\")\n",
    "print(f\"# Words in 30 Res corpus: {sum(num_tokens_res)} | Med.: {statistics.median(num_tokens_res)}\")\n",
    "print(f\"# Tables in 30 Res corpus: {num_tables_res} | Mean: {num_tables_res/30:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_sort_key(item):\n",
    "    order = {'T': 1, 'E': 2, 'A': 3, 'N': 4}\n",
    "    tag_id, _ = item.split(\"\\t\", 1)\n",
    "    return order[tag_id[0]], int(tag_id[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For full text annotation, split annotation for 1) abstract part and 2) Result part\n",
    "\n",
    "# To quantify our annotations\n",
    "entity_type_stats = defaultdict(int)\n",
    "relation_type_stats = defaultdict(int)\n",
    "factuality_type_stats = defaultdict(dict)\n",
    "normalization_stats = defaultdict(dict)\n",
    "\n",
    "disjoint_entity_stats = defaultdict(int)\n",
    "entity_pair_4_directional_rel_stats = defaultdict(dict)\n",
    "entity_pair_4_bidirectional_rel_stats = defaultdict(dict)\n",
    "\n",
    "entity_per_doc_abs_stats = {}\n",
    "entity_per_doc_result_stats = {}\n",
    "trigger_per_doc_abs_stats = {}\n",
    "trigger_per_doc_result_stats = {}\n",
    "relation_per_doc_abs_stats = {}\n",
    "relation_per_doc_result_stats = {}\n",
    "factuality_per_doc_abs_stats = {}\n",
    "factuality_per_doc_result_stats = {}\n",
    "normalization_per_doc_abs_stats = {}\n",
    "normalization_per_doc_result_stats = {}\n",
    "\n",
    "IGNORED_CLASSES = [\"Methodology\", \"Biospecimen\", \"Population\"]\n",
    "# IGNORED_CLASSES = []\n",
    "\n",
    "# This is to for 3-way classification of certainty: Factual / Unknown / Negated\n",
    "factuality_unknown = ['Probable', 'Possible', 'Doubtful']\n",
    "# factuality_unknown = []\n",
    "\n",
    "for ann in ann_files:\n",
    "    \n",
    "    doc_key = ann.strip(\".ann\")\n",
    "    print(f\"### {doc_key} ###\")\n",
    "    \n",
    "    entity_per_doc_abs_stats[doc_key] = defaultdict(int)\n",
    "    entity_per_doc_result_stats[doc_key] = defaultdict(int)\n",
    "    trigger_per_doc_abs_stats[doc_key] = defaultdict(int)\n",
    "    trigger_per_doc_result_stats[doc_key] = defaultdict(int)\n",
    "    relation_per_doc_abs_stats[doc_key] = defaultdict(int)\n",
    "    relation_per_doc_result_stats[doc_key] = defaultdict(int)\n",
    "    factuality_per_doc_abs_stats[doc_key] = defaultdict(int)\n",
    "    factuality_per_doc_result_stats[doc_key] = defaultdict(int)\n",
    "    normalization_per_doc_abs_stats[doc_key] = defaultdict(int)\n",
    "    normalization_per_doc_result_stats[doc_key] = defaultdict(int)\n",
    "    \n",
    "    entities = {}\n",
    "    relations = {}\n",
    "    modalities = {}\n",
    "    normalizations = {}\n",
    "    \n",
    "    duplicates = defaultdict(list)\n",
    "    title_abs_doc = \"\"\n",
    "    result_doc = \"\"\n",
    "\n",
    "    with open(os.path.join(raw_file_path, ann), encoding=\"utf-8\") as f:\n",
    "        \n",
    "        lines = f.read().splitlines()\n",
    "        lines = sorted(lines, key=custom_sort_key)  # T -> E -> A -> N\n",
    "\n",
    "        # Tag information\n",
    "        title_abs_tags = []\n",
    "        result_tags = []\n",
    "        \n",
    "        for line in lines:\n",
    "            if line.startswith(\"T\"):\n",
    "                entity_id, entity_annotation, entity_mention = line.split(\"\\t\")\n",
    "                \n",
    "                entity_id = entity_id.strip()\n",
    "                entity_annotation = entity_annotation.strip()\n",
    "                entity_mention = entity_mention.strip()\n",
    "                \n",
    "                annotation_elements = entity_annotation.split(\";\")\n",
    "                entity_type, *first_offset_pair = annotation_elements[0].split()\n",
    "                \n",
    "                entity_type_stats[entity_type] += 1\n",
    "                \n",
    "                # Re-categorization: Need to be fixed following the dataset versions\n",
    "                if entity_type in IGNORED_CLASSES:\n",
    "                    continue\n",
    "                \n",
    "                offset_pairs = [first_offset_pair] + [\n",
    "                    offset_pair.split() for offset_pair in annotation_elements[1:]\n",
    "                ]\n",
    "                \n",
    "                if len(offset_pairs) > 1:\n",
    "#                     print(f\"## Discontinuous entity found: {entity_id} in {doc_key}\")\n",
    "                    disjoint_entity_stats[entity_type] += 1\n",
    "                    \n",
    "                value = {\n",
    "                    \"type\": entity_type,\n",
    "                    \"spans\": offset_pairs,\n",
    "                    \"mention\": entity_mention\n",
    "                }\n",
    "                \n",
    "                # Check duplication\n",
    "                if value in entities.values():\n",
    "                    for k, v in entities.items():\n",
    "                        if v == value:\n",
    "                            duplicates[k].append(entity_id)\n",
    "                            entity_type_stats[entity_type] -= 1\n",
    "                            print(f\"## {entity_id} was replaced by {k} in {doc_key}\")\n",
    "                            break\n",
    "                    continue\n",
    "                else:\n",
    "                    entities[entity_id] = value\n",
    "\n",
    "                abs_offset_pairs = []\n",
    "                result_offset_pairs = []\n",
    "                # Split into abs & result\n",
    "                if doc_key.startswith('PMC') and ('Res_offset' in offset_dict[doc_key]):                  \n",
    "                    for span_pair in offset_pairs:\n",
    "                        start, end = span_pair\n",
    "                        start, end = int(start), int(end)\n",
    "                        # Title-abs part\n",
    "                        if start < offset_dict[doc_key]['Res_offset']:\n",
    "                            abs_offset_pairs.append(f\"{start} {end}\")\n",
    "                        # Result part -> we need to modify the span information\n",
    "                        else:\n",
    "                            OFFSET = offset_dict[doc_key]['Res_offset']\n",
    "                            start -= OFFSET\n",
    "                            end -= OFFSET\n",
    "                            result_offset_pairs.append(f\"{start} {end}\")\n",
    "\n",
    "                    if not result_offset_pairs:        \n",
    "                        new_offset = \";\".join(abs_offset_pairs)\n",
    "                        new_annotation = \" \".join([entity_type, new_offset])\n",
    "                        newline = \"\\t\".join([entity_id, new_annotation, entity_mention])                      \n",
    "                        title_abs_doc += newline + \"\\n\"\n",
    "                        title_abs_tags.append(entity_id)\n",
    "                        if not entity_type.isupper():\n",
    "                            entity_per_doc_abs_stats[doc_key][entity_type] += 1\n",
    "                        else:\n",
    "                            trigger_per_doc_abs_stats[doc_key][entity_type] += 1\n",
    "                    else:\n",
    "                        new_offset = \";\".join(result_offset_pairs)\n",
    "                        new_annotation = \" \".join([entity_type, new_offset])\n",
    "                        newline = \"\\t\".join([entity_id, new_annotation, entity_mention])\n",
    "                        result_doc += newline + \"\\n\"\n",
    "                        result_tags.append(entity_id)\n",
    "                        if not entity_type.isupper():\n",
    "                            entity_per_doc_result_stats[doc_key][entity_type] += 1\n",
    "                        else:\n",
    "                            trigger_per_doc_result_stats[doc_key][entity_type] += 1\n",
    "                else:\n",
    "                    for span_pair in offset_pairs:\n",
    "                        start, end = span_pair\n",
    "                        abs_offset_pairs.append(f\"{start} {end}\")\n",
    "                        \n",
    "                    new_offset = \";\".join(abs_offset_pairs)\n",
    "                    new_annotation = \" \".join([entity_type, new_offset])\n",
    "                    newline = \"\\t\".join([entity_id, new_annotation, entity_mention])    \n",
    "                    title_abs_doc += newline + \"\\n\"\n",
    "                    title_abs_tags.append(entity_id)\n",
    "                    if not entity_type.isupper():\n",
    "                        entity_per_doc_abs_stats[doc_key][entity_type] += 1\n",
    "                    else:\n",
    "                        trigger_per_doc_abs_stats[doc_key][entity_type] += 1\n",
    "                                  \n",
    "            elif line.startswith(\"E\"):\n",
    "                relation_id, relation_annotation = line.split(\"\\t\")\n",
    "\n",
    "                relation_id = relation_id.strip()\n",
    "                relation_annotation = relation_annotation.strip()\n",
    "\n",
    "                assert len(relation_annotation.split()) == 3, f\"##{relation_id} in {doc_key} without two args\"\n",
    "            \n",
    "                trigger, arg1, arg2 = relation_annotation.split()\n",
    "\n",
    "                trigger_type, trigger_id = trigger.split(\":\")\n",
    "                arg1_role, arg1_id = arg1.split(\":\")\n",
    "                arg2_role, arg2_id = arg2.split(\":\")\n",
    "                \n",
    "                if entities[arg1_id][\"type\"] in [\"Methodology\", \"Biospecimen\", \"Population\"]:\n",
    "                    print(line)\n",
    "                    print(arg1_id, \":\", entities[arg1_id][\"type\"])\n",
    "                if entities[arg2_id][\"type\"] in [\"Methodology\", \"Biospecimen\", \"Population\"]:\n",
    "                    print(line)\n",
    "                    print(arg2_id, \":\", entities[arg2_id][\"type\"])\n",
    "                \n",
    "                assert arg1_id in entities, print(line)\n",
    "                assert arg2_id in entities, print(line)\n",
    "                assert sorted([arg1_role, arg2_role]) in [[\"Agent\", \"Theme\"], [\"Theme\", \"Theme2\"]], print(line)\n",
    "                \n",
    "                relation_type_stats[trigger_type] += 1\n",
    "                \n",
    "                bidirectional = False\n",
    "                if arg1_role == \"Agent\":\n",
    "                    agent_entity_type = entities[arg1_id][\"type\"]\n",
    "                    theme_entity_type = entities[arg2_id][\"type\"]\n",
    "                elif arg2_role == \"Agent\":\n",
    "                    agent_entity_type = entities[arg2_id][\"type\"]\n",
    "                    theme_entity_type = entities[arg1_id][\"type\"]\n",
    "                else:  # bidirectional rel\n",
    "                    bidirectional = True\n",
    "                    if not trigger_type in [\"INTERACTS_WITH\", \"ASSOCIATED_WITH\",\n",
    "                                           \"POS_ASSOCIATED_WITH\", \"NEG_ASSOCIATED_WITH\"]:\n",
    "                        print(\"## Inappropriate Arguments\")\n",
    "                        print(line, \"\\n\")\n",
    "                        \n",
    "                    if arg1_role == \"Theme\":\n",
    "                        agent_entity_type = entities[arg1_id][\"type\"]\n",
    "                        theme_entity_type = entities[arg2_id][\"type\"]\n",
    "                    elif arg1_role == \"Theme2\":\n",
    "                        agent_entity_type = entities[arg2_id][\"type\"]\n",
    "                        theme_entity_type = entities[arg1_id][\"type\"]\n",
    "                        \n",
    "                if not bidirectional:\n",
    "                    if trigger_type == \"INTERACTS_WITH\":\n",
    "                        print(\"## Inappropriate Arguments\")\n",
    "                        print(line, \"\\n\")\n",
    "                \n",
    "                entity_pair = (agent_entity_type, theme_entity_type)\n",
    "                if not bidirectional:\n",
    "                    if trigger_type not in entity_pair_4_directional_rel_stats[entity_pair]:\n",
    "                        entity_pair_4_directional_rel_stats[entity_pair][trigger_type] = [(doc_key, relation_id)]\n",
    "                    else:\n",
    "                        entity_pair_4_directional_rel_stats[entity_pair][trigger_type].append([(doc_key, relation_id)])\n",
    "                else:\n",
    "                    if trigger_type not in entity_pair_4_bidirectional_rel_stats[entity_pair]:\n",
    "                        entity_pair_4_bidirectional_rel_stats[entity_pair][trigger_type] = [(doc_key, relation_id)]\n",
    "                    else:    \n",
    "                        entity_pair_4_bidirectional_rel_stats[entity_pair][trigger_type].append([(doc_key, relation_id)])\n",
    "                \n",
    "                # replace removed entity ids\n",
    "                for key_id, sub_ids in duplicates.items():\n",
    "                    if trigger_id in sub_ids:\n",
    "                        print(f\"## REL: {trigger_id} was replaced by {key_id} in {doc_key}\")\n",
    "                        trigger_id = key_id\n",
    "                    if arg1_id in sub_ids:\n",
    "                        print(f\"## REL: {arg1_id} was replaced by {key_id} in {doc_key}\")\n",
    "                        arg1_id = key_id\n",
    "                    if arg2_id in sub_ids:\n",
    "                        print(f\"## REL: {arg2_id} was replaced by {key_id} in {doc_key}\")\n",
    "                        arg2_id = key_id\n",
    "\n",
    "                args = [\n",
    "                    {\"role\": arg1_role, \"id\": arg1_id},\n",
    "                    {\"role\": arg2_role, \"id\": arg2_id}\n",
    "                ]\n",
    "\n",
    "                value = {\n",
    "                    \"trigger_type\": trigger_type,\n",
    "                    \"trigger_id\": trigger_id,\n",
    "                    \"args\": args,\n",
    "                }\n",
    "                \n",
    "                # check duplication\n",
    "                if value in relations.values():\n",
    "                    for k, v in relations.items():\n",
    "                        if v == value:\n",
    "                            duplicates[k].append(relation_id)\n",
    "                            print(f\"## REL: {relation_id} was replaced by {k} in {doc_key}\")\n",
    "                            break\n",
    "                    continue\n",
    "                else:\n",
    "                    relations[relation_id] = value\n",
    "                \n",
    "                new_trigger = [f\"{trigger_type}:{trigger_id}\"]\n",
    "                new_args = [f\"{arg['role']}:{arg['id']}\" for arg in args]\n",
    "                new_relation_annotation = \" \".join(new_trigger + new_args)\n",
    "                newline = \"\\t\".join([relation_id, new_relation_annotation])\n",
    "                \n",
    "                if trigger_id in title_abs_tags:\n",
    "                    title_abs_doc += newline + \"\\n\"\n",
    "                    title_abs_tags.append(relation_id)\n",
    "                    relation_per_doc_abs_stats[doc_key][trigger_type] += 1\n",
    "                else:\n",
    "                    result_doc += newline + \"\\n\"\n",
    "                    result_tags.append(relation_id)\n",
    "                    relation_per_doc_result_stats[doc_key][trigger_type] += 1\n",
    "                \n",
    "                \n",
    "            elif line.startswith(\"A\"):\n",
    "                modal_id, modal_annotation = line.split(\"\\t\")\n",
    "                _, reference_id, modal_type = modal_annotation.split(\" \")\n",
    "                \n",
    "                assert modal_type in [\"Probable\", \"Possible\", \"Negated\", \"Doubtful\", \"Unknown\"], print(doc_key, modal_id)\n",
    "                \n",
    "                if modal_type in factuality_unknown:\n",
    "                    modal_type = 'Unknown'\n",
    "                \n",
    "                for key_id, sub_ids in duplicates.items():\n",
    "                    if reference_id in sub_ids:\n",
    "                        print(f\"## MOD: {reference_id} was replaced by {key_id} in {doc_key}\")\n",
    "                        reference_id = key_id\n",
    "                \n",
    "                value = {\"type\": modal_type, \"reference_id\": reference_id}\n",
    "                \n",
    "                trigger_type = relations[reference_id][\"trigger_type\"]\n",
    "                if modal_type not in factuality_type_stats[trigger_type]:\n",
    "                    factuality_type_stats[trigger_type][modal_type] = 1\n",
    "                else:\n",
    "                    factuality_type_stats[trigger_type][modal_type] += 1\n",
    "                \n",
    "                # check duplication\n",
    "                if value in modalities.values():\n",
    "                    for k, v in modalities.items():\n",
    "                        if v == value:\n",
    "                            duplicates[k].append(modal_id)\n",
    "                            print(f\"## MOD: {modal_id} was replaced by {k} in {doc_key}\")\n",
    "                            break\n",
    "                else:\n",
    "                    modalities[modal_id] = value\n",
    "                    \n",
    "                new_modal_annotation = \" \".join([modal_type, reference_id])\n",
    "                newline = \"\\t\".join([modal_id, new_modal_annotation])\n",
    "                    \n",
    "                if reference_id in title_abs_tags:\n",
    "                    title_abs_doc += newline + \"\\n\"\n",
    "                    factuality_per_doc_abs_stats[doc_key][modal_type] += 1\n",
    "                else:\n",
    "                    result_doc += newline + \"\\n\"\n",
    "                    factuality_per_doc_result_stats[doc_key][modal_type] += 1\n",
    "                    \n",
    "            \n",
    "            elif line.startswith(\"N\"):\n",
    "                normalization_id, normalization_annotation, normalized_name = line.split(\"\\t\")\n",
    "                _, reference_id, normalization_src_id = normalization_annotation.split(\" \")\n",
    "                ontology_src, ontology_id = normalization_src_id.split(\":\")\n",
    "                \n",
    "                # Just to process edge case\n",
    "                if not ontology_src.isupper():\n",
    "                    ontology_src = ontology_src.upper()\n",
    "                \n",
    "                if reference_id in entities:  # Consider target entity types only\n",
    "                    entity_type = entities[reference_id][\"type\"]\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "                if ontology_src not in normalization_stats[entity_type]:\n",
    "                    normalization_stats[entity_type][ontology_src] = 1\n",
    "                else:\n",
    "                    normalization_stats[entity_type][ontology_src] += 1\n",
    "                \n",
    "                for key_id, sub_ids in duplicates.items():\n",
    "                    if reference_id in sub_ids:\n",
    "                        print(f\"## MOD: {reference_id} was replaced by {key_id} in {doc_key}\")\n",
    "                        reference_id = key_id\n",
    "                        \n",
    "                value = {\n",
    "                    \"source\": ontology_src,\n",
    "                    \"id\": ontology_id,\n",
    "                    \"reference_id\": reference_id,\n",
    "                    \"normalized_name\": normalized_name\n",
    "                }\n",
    "\n",
    "                if value in normalizations.values():\n",
    "                    for k, v in normalizations.items():\n",
    "                        if v == value:\n",
    "                            duplicates[k].append(normalization_id)\n",
    "                            print(f\"## MOD: {normalization_id} was replaced by {k} in {doc_key}\")\n",
    "                            break\n",
    "                else:\n",
    "                    normalizations[normalization_id] = value\n",
    "                    \n",
    "                new_norm_annotation = \" \".join([normalization_src_id, reference_id])\n",
    "                newline = \"\\t\".join([normalization_id, new_norm_annotation, normalized_name])\n",
    "                \n",
    "                if reference_id in title_abs_tags:\n",
    "                    title_abs_doc += newline + \"\\n\"\n",
    "                    normalization_per_doc_abs_stats[doc_key][ontology_src] += 1\n",
    "                else:\n",
    "                    result_doc += newline + \"\\n\"\n",
    "                    normalization_per_doc_result_stats[doc_key][ontology_src] += 1\n",
    "\n",
    "    # Update 'Factual'\n",
    "    valid_reference_ids = []\n",
    "    for modal_id in modalities:\n",
    "        valid_reference_ids.append(modalities[modal_id][\"reference_id\"])\n",
    "    \n",
    "    if modalities.keys():\n",
    "        last_modal_int = int(sorted(modalities.keys(), key=lambda x: int(x[1:]))[-1][1:])\n",
    "    else:\n",
    "        last_modal_int = 0\n",
    "\n",
    "    for trigger_id in relations:\n",
    "        if trigger_id not in valid_reference_ids:\n",
    "            valid_reference_ids.append(trigger_id)\n",
    "            last_modal_int += 1\n",
    "            modal_id = f\"A{str(last_modal_int)}\"\n",
    "            modal_type = \"Factual\"\n",
    "            value = {\"type\": modal_type, \"reference_id\": trigger_id}\n",
    "\n",
    "            trigger_type = relations[trigger_id][\"trigger_type\"]\n",
    "            if modal_type not in factuality_type_stats[trigger_type]:\n",
    "                factuality_type_stats[trigger_type][modal_type] = 1\n",
    "            else:\n",
    "                factuality_type_stats[trigger_type][modal_type] += 1\n",
    "\n",
    "            # check duplication\n",
    "            if value in modalities.values():\n",
    "                for k, v in modalities.items():\n",
    "                    if v == value:\n",
    "                        duplicates[k].append(modal_id)\n",
    "                        print(f\"## MOD: {modal_id} was replaced by {k} in {doc_key}\")\n",
    "                        break\n",
    "            else:\n",
    "                modalities[modal_id] = value\n",
    "\n",
    "            new_modal_annotation = \" \".join([modal_type, trigger_id])\n",
    "            newline = \"\\t\".join([modal_id, new_modal_annotation])\n",
    "\n",
    "            if trigger_id in title_abs_tags:\n",
    "                title_abs_doc += newline + \"\\n\"\n",
    "                factuality_per_doc_abs_stats[doc_key][modal_type] += 1\n",
    "            else:\n",
    "                result_doc += newline + \"\\n\"\n",
    "                factuality_per_doc_result_stats[doc_key][modal_type] += 1\n",
    "                    \n",
    "    title_abs_fname = doc_key + \".ann\"\n",
    "    with open(os.path.join(title_abs_path, title_abs_fname), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(title_abs_doc)\n",
    "        print(f\"{title_abs_fname} was saved in Title-abs folder\")\n",
    "\n",
    "    if result_doc:\n",
    "        result_fname = doc_key + \"-RESULT.ann\"   \n",
    "        with open(os.path.join(results_path, result_fname), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(result_doc)\n",
    "            print(f\"{result_fname} was saved in Result folder\")\n",
    "    \n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Food': 1047, 'Metabolite': 1181, 'Population': 1235, 'Measurement': 814, 'Microorganism': 1938, 'Chemical': 1119, 'Methodology': 951, 'Physiology': 1716, 'Nutrient': 2045, 'Biospecimen': 307, 'Disease': 604, 'DietPattern': 851, 'Enzyme': 239, 'Gene': 173, 'DiversityMetric': 231}\n",
      "\n",
      "{'IMPROVES': 179, 'WORSENS': 25, 'HAS_COMPONENT': 132, 'AFFECTS': 402, 'INCREASES': 596, 'DECREASES': 440, 'ASSOCIATED_WITH': 148, 'INTERACTS_WITH': 21, 'POS_ASSOCIATED_WITH': 145, 'PREDISPOSES': 15, 'CAUSES': 24, 'PREVENTS': 27, 'NEG_ASSOCIATED_WITH': 83}\n",
      "\n",
      "# of Entity: 14451, # of Trigger: 2237\n"
     ]
    }
   ],
   "source": [
    "entity_stats = {k:v for k, v in entity_type_stats.items() if not k.isupper()}\n",
    "trigger_stats = {k:v for k, v in entity_type_stats.items() if k.isupper()}\n",
    "\n",
    "print(entity_stats)\n",
    "print()\n",
    "print(trigger_stats)\n",
    "\n",
    "num_entity = sum(entity_stats.values())\n",
    "num_trigger = sum(trigger_stats.values())\n",
    "        \n",
    "print()        \n",
    "print(f\"# of Entity: {num_entity}, # of Trigger: {num_trigger}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'IMPROVES': 257, 'HAS_COMPONENT': 218, 'WORSENS': 36, 'AFFECTS': 942, 'INCREASES': 1128, 'DECREASES': 774, 'ASSOCIATED_WITH': 250, 'INTERACTS_WITH': 29, 'POS_ASSOCIATED_WITH': 282, 'PREDISPOSES': 31, 'CAUSES': 33, 'PREVENTS': 39, 'NEG_ASSOCIATED_WITH': 187})\n",
      "\n",
      "# of Relation: 4206\n"
     ]
    }
   ],
   "source": [
    "print(relation_type_stats)\n",
    "print()        \n",
    "print(f\"# of Relation: {sum(relation_type_stats.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'dict'>, {'WORSENS': {'Negated': 6, 'Factual': 28, 'Unknown': 2}, 'AFFECTS': {'Negated': 422, 'Unknown': 283, 'Factual': 237}, 'IMPROVES': {'Factual': 205, 'Unknown': 44, 'Negated': 8}, 'HAS_COMPONENT': {'Factual': 215, 'Negated': 3}, 'INCREASES': {'Factual': 1117, 'Negated': 2, 'Unknown': 9}, 'DECREASES': {'Factual': 757, 'Negated': 9, 'Unknown': 8}, 'ASSOCIATED_WITH': {'Factual': 208, 'Negated': 21, 'Unknown': 21}, 'INTERACTS_WITH': {'Factual': 28, 'Unknown': 1}, 'POS_ASSOCIATED_WITH': {'Factual': 266, 'Negated': 2, 'Unknown': 14}, 'PREDISPOSES': {'Unknown': 6, 'Factual': 25}, 'CAUSES': {'Factual': 32, 'Negated': 1}, 'PREVENTS': {'Unknown': 15, 'Factual': 24}, 'NEG_ASSOCIATED_WITH': {'Factual': 187}})\n"
     ]
    }
   ],
   "source": [
    "print(factuality_type_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'Negated': 474, 'Factual': 3329, 'Unknown': 403})\n",
      "4206\n"
     ]
    }
   ],
   "source": [
    "factuality_count_dict = defaultdict(int)    \n",
    "for rel_type, factualities in factuality_type_stats.items():\n",
    "    for fact_type, cnt in factualities.items():\n",
    "        factuality_count_dict[fact_type] += cnt\n",
    "        \n",
    "print(factuality_count_dict)\n",
    "print(sum(factuality_count_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'dict'>, {'Food': {'MESH': 124, 'FOODON': 491, 'CHEBI': 12, 'NCIT': 215, 'OCHV': 50}, 'Metabolite': {'CHEBI': 1000, 'NCIT': 24, 'MESH': 73}, 'Microorganism': {'NCBITAXON': 1780, 'OCHV': 19, 'NCIT': 60, 'MESH': 5}, 'Chemical': {'NCIT': 486, 'CHEBI': 398, 'MESH': 148, 'OCHV': 28}, 'Physiology': {'NCIT': 658, 'OCHV': 156, 'MESH': 259, 'CHEBI': 10}, 'Nutrient': {'FOODON': 34, 'NCIT': 602, 'CHEBI': 732, 'MESH': 305, 'NCBITAXON': 12, 'OCHV': 6}, 'Measurement': {'NCIT': 426, 'OCHV': 39, 'MESH': 93, 'CHEBI': 1}, 'Disease': {'NCIT': 472, 'MESH': 119, 'OCHV': 4}, 'DietPattern': {'OCHV': 225, 'MESH': 119, 'NCIT': 182, 'CHEBI': 1, 'FOODON': 72}, 'Enzyme': {'NCIT': 77, 'MESH': 120, 'CHEBI': 1}, 'Gene': {'GENE': 121, 'NCIT': 10}, 'DiversityMetric': {'NCIT': 24}})\n"
     ]
    }
   ],
   "source": [
    "print(normalization_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'MESH': 1365, 'FOODON': 597, 'CHEBI': 2155, 'NCIT': 3236, 'OCHV': 527, 'NCBITAXON': 1792, 'GENE': 121})\n",
      "9793\n"
     ]
    }
   ],
   "source": [
    "normalization_count_dict = defaultdict(int)    \n",
    "for ent_type, sources in normalization_stats.items():\n",
    "    for source, cnt in sources.items():\n",
    "        normalization_count_dict[source] += cnt\n",
    "        \n",
    "print(normalization_count_dict)\n",
    "print(sum(normalization_count_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'Food': 15, 'Physiology': 90, 'Measurement': 19, 'Microorganism': 7, 'Metabolite': 32, 'Nutrient': 18, 'DietPattern': 16, 'Gene': 7, 'DiversityMetric': 5, 'Enzyme': 4, 'AFFECTS': 1, 'Chemical': 6}) \n",
      "\n",
      "% of disjoint entities: 1.52%\n"
     ]
    }
   ],
   "source": [
    "print(disjoint_entity_stats, \"\\n\")\n",
    "print(f\"% of disjoint entities: {round(sum(disjoint_entity_stats.values())/num_entity*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6388 5570 1536 701 2831 1375\n",
      "\n",
      "defaultdict(<class 'int'>, {'Negated': 256, 'Unknown': 388, 'Factual': 2187}) 2831\n",
      "defaultdict(<class 'int'>, {'Negated': 218, 'Factual': 1142, 'Unknown': 15}) 1375\n",
      "\n",
      "defaultdict(<class 'int'>, {'MESH': 754, 'CHEBI': 1126, 'FOODON': 328, 'NCBITAXON': 724, 'NCIT': 1675, 'OCHV': 306, 'GENE': 34}) 4947\n",
      "defaultdict(<class 'int'>, {'CHEBI': 1029, 'NCBITAXON': 1068, 'OCHV': 221, 'NCIT': 1561, 'MESH': 611, 'GENE': 87, 'FOODON': 269}) 4846\n"
     ]
    }
   ],
   "source": [
    "n_entity_abs = 0\n",
    "for k, v in entity_per_doc_abs_stats.items():\n",
    "    n_entity_abs += sum(v.values())\n",
    "    \n",
    "n_entity_res = 0\n",
    "for k, v in entity_per_doc_result_stats.items():\n",
    "    n_entity_res += sum(v.values())\n",
    "    \n",
    "n_trigger_abs = 0\n",
    "for k, v in trigger_per_doc_abs_stats.items():\n",
    "    n_trigger_abs += sum(v.values())\n",
    "    \n",
    "n_trigger_res = 0\n",
    "for k, v in trigger_per_doc_result_stats.items():\n",
    "    n_trigger_res += sum(v.values())\n",
    "    \n",
    "n_relation_abs = 0\n",
    "for k, v in relation_per_doc_abs_stats.items():\n",
    "    n_relation_abs += sum(v.values())\n",
    "    \n",
    "n_relation_res = 0\n",
    "for k, v in relation_per_doc_result_stats.items():\n",
    "    n_relation_res += sum(v.values())\n",
    "    \n",
    "n_fact_abs_dict = defaultdict(int)\n",
    "for k, v in factuality_per_doc_abs_stats.items():\n",
    "    for k_, v_ in v.items():\n",
    "        n_fact_abs_dict[k_] += v_\n",
    "    \n",
    "n_fact_res_dict = defaultdict(int)\n",
    "for k, v in factuality_per_doc_result_stats.items():\n",
    "    for k_, v_ in v.items():\n",
    "        n_fact_res_dict[k_] += v_\n",
    "    \n",
    "n_norm_abs_dict = defaultdict(int)\n",
    "for k, v in normalization_per_doc_abs_stats.items():\n",
    "    for k_, v_ in v.items():\n",
    "        n_norm_abs_dict[k_] += v_\n",
    "    \n",
    "n_norm_res_dict = defaultdict(int)\n",
    "for k, v in normalization_per_doc_result_stats.items():\n",
    "    for k_, v_ in v.items():\n",
    "        n_norm_res_dict[k_] += v_\n",
    "    \n",
    "print(n_entity_abs, n_entity_res, n_trigger_abs, n_trigger_res, n_relation_abs, n_relation_res)\n",
    "print()\n",
    "print(n_fact_abs_dict, sum(n_fact_abs_dict.values()))\n",
    "print(n_fact_res_dict, sum(n_fact_res_dict.values()))\n",
    "print()\n",
    "print(n_norm_abs_dict, sum(n_norm_abs_dict.values()))\n",
    "print(n_norm_res_dict, sum(n_norm_res_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_abs_stats = {}\n",
    "for k, v in relation_per_doc_abs_stats.items():\n",
    "    doc_abs_stats[k] = v\n",
    "    doc_abs_stats[k].update(entity_per_doc_abs_stats[k])\n",
    "    doc_abs_stats[k].update(factuality_per_doc_abs_stats[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset and store the filenames\n",
    "- Stratify classes with 7:1:2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc_count_per_ner_rel = {}\n",
    "for label in relation_type_stats:\n",
    "    doc_count_per_ner_rel[label] = defaultdict(int)\n",
    "for label in entity_type_stats:\n",
    "    if label not in IGNORED_CLASSES:\n",
    "        doc_count_per_ner_rel[label] = defaultdict(int)\n",
    "    \n",
    "for doc_id, counts in doc_abs_stats.items():\n",
    "    for label, cnt in counts.items():\n",
    "        if label in ['Factual', 'Negated', 'Unknown']:\n",
    "            continue\n",
    "        doc_count_per_ner_rel[label][doc_id] = cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def manual_split(inv_stats):\n",
    "\n",
    "    predefined_split = {\n",
    "        'train': [],\n",
    "        'dev': ['19224658', '22099384', '23576043', '29665619', '30916575', \n",
    "                '34004416', '34143954', '34256014', '35654220', '36067589', \n",
    "                '36076452', 'PMC4994979', 'PMC5131798', 'PMC8839280'],\n",
    "        'test': ['12004211', '20113315', '20591206', '25527750', '27052535', \n",
    "                 '31085979', '35613674', '35643872', '35748920', '35833889', \n",
    "                 'PMC3869907', 'PMC8942430', 'PMC9182596', 'PMC9183096', 'PMC9199182', \n",
    "                 'PMC9239261', 'PMC9279853'],\n",
    "    }\n",
    "    \n",
    "    rare_labels = ['WORSENS', 'INTERACTS_WITH', 'PREDISPOSES', 'CAUSES', 'PREVENTS',\n",
    "                  'Enzyme', 'Gene', 'DiversityMetric']\n",
    "    \n",
    "    for label in rare_labels:\n",
    "        for fn in inv_stats[label]:\n",
    "            if fn not in predefined_split['train'] + predefined_split['dev'] + predefined_split['test']:\n",
    "                predefined_split['train'].append(fn)\n",
    "    \n",
    "    return predefined_split\n",
    "\n",
    "def split_dataset(stats, inv_stats, result_stats, seed=42):\n",
    "    \n",
    "    predefined_split = manual_split(inv_stats)\n",
    "    \n",
    "    filenames = list(stats.keys())\n",
    "    \n",
    "    total_rel = 0\n",
    "    for k, v in stats.items():\n",
    "        total_rel += sum(v.values())\n",
    "    \n",
    "    train_rel = 0\n",
    "    dev_rel = 0\n",
    "    test_rel = 0\n",
    "    train_res_rel = 0\n",
    "    \n",
    "    train_rel_dict = defaultdict(int)\n",
    "    dev_rel_dict = defaultdict(int)\n",
    "    test_rel_dict = defaultdict(int)\n",
    "    result_rel_dict = defaultdict(int)\n",
    "    \n",
    "    split_fn = {\"train\":[], \"dev\":[], \"test\":[], \"train_result\":[]}\n",
    "    for key, files in predefined_split.items():\n",
    "        for fn in files:\n",
    "            if key == \"train\":\n",
    "                train_rel += sum(stats[fn].values())\n",
    "                for k, v in stats[fn].items():\n",
    "                    train_rel_dict[k] += v\n",
    "            elif key == \"dev\":\n",
    "                dev_rel += sum(stats[fn].values())\n",
    "                for k, v in stats[fn].items():\n",
    "                    dev_rel_dict[k] += v\n",
    "            else:\n",
    "                test_rel += sum(stats[fn].values())\n",
    "                for k, v in stats[fn].items():\n",
    "                    test_rel_dict[k] += v\n",
    "            split_fn[key].append(fn)\n",
    "            filenames.remove(fn)\n",
    "    \n",
    "#     print(f\"## {len(filenames)} are left after processing predefined split for dev and test ##\")\n",
    "    \n",
    "    random.seed(seed)\n",
    "    random.shuffle(filenames)\n",
    "    \n",
    "    for fn in filenames:\n",
    "        if dev_rel < total_rel*0.1:\n",
    "            split_fn['dev'].append(fn)\n",
    "            dev_rel += sum(stats[fn].values())\n",
    "            for k, v in stats[fn].items():\n",
    "                dev_rel_dict[k] += v\n",
    "        elif test_rel < total_rel*0.2:\n",
    "            split_fn['test'].append(fn)\n",
    "            test_rel += sum(stats[fn].values())\n",
    "            for k, v in stats[fn].items():\n",
    "                test_rel_dict[k] += v\n",
    "        else:\n",
    "            split_fn['train'].append(fn)\n",
    "            train_rel += sum(stats[fn].values())\n",
    "            for k, v in stats[fn].items():\n",
    "                train_rel_dict[k] += v\n",
    "                \n",
    "    train_rel_dict = dict(sorted([(k, v) for k, v in train_rel_dict.items()], key=lambda x: x[0]))\n",
    "    dev_rel_dict = dict(sorted([(k, v) for k, v in dev_rel_dict.items()], key=lambda x: x[0]))\n",
    "    test_rel_dict = dict(sorted([(k, v) for k, v in test_rel_dict.items()], key=lambda x: x[0]))\n",
    "            \n",
    "    # For RESULT part\n",
    "    for fn, v in result_stats.items():\n",
    "        if v:\n",
    "            split_fn['train_result'].append(fn)\n",
    "            train_res_rel += sum(result_stats[fn].values())\n",
    "            for k_, v_ in result_stats[fn].items():\n",
    "                result_rel_dict[k_] += v_\n",
    "    \n",
    "    result_rel_dict = dict(sorted([(k, v) for k, v in result_rel_dict.items()], key=lambda x: x[0]))\n",
    "\n",
    "    return split_fn, train_rel_dict, dev_rel_dict, test_rel_dict, result_rel_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "flag = True\n",
    "for seed in range(5):\n",
    "    split_filenames, train_rel_dict, dev_rel_dict, test_rel_dict, result_rel_dict = split_dataset(\n",
    "        doc_abs_stats, doc_count_per_ner_rel, relation_per_doc_result_stats, seed=seed\n",
    "    )\n",
    "    try:\n",
    "        for k in train_rel_dict:\n",
    "            total = train_rel_dict[k]+dev_rel_dict[k]+test_rel_dict[k]\n",
    "            if train_rel_dict[k]/total < 0.6 or dev_rel_dict[k]/total < 0.03:\n",
    "                flag = False\n",
    "                break\n",
    "    except:\n",
    "#         print(k)\n",
    "        flag = False\n",
    "        continue\n",
    "    if flag:\n",
    "        print(\"## All labels satisfy the threshold for dataset split ##\")\n",
    "        print(seed)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in split_filenames.items():\n",
    "    v.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AFFECTS': 442, 'ASSOCIATED_WITH': 123, 'CAUSES': 21, 'Chemical': 343, 'DECREASES': 322, 'DietPattern': 355, 'Disease': 245, 'DiversityMetric': 42, 'Enzyme': 46, 'Factual': 1566, 'Food': 447, 'Gene': 48, 'HAS_COMPONENT': 107, 'IMPROVES': 145, 'INCREASES': 603, 'INTERACTS_WITH': 19, 'Measurement': 258, 'Metabolite': 468, 'Microorganism': 523, 'NEG_ASSOCIATED_WITH': 59, 'Negated': 176, 'Nutrient': 801, 'POS_ASSOCIATED_WITH': 107, 'PREDISPOSES': 18, 'PREVENTS': 28, 'Physiology': 771, 'Unknown': 268, 'WORSENS': 16} \n",
      "\n",
      "{'AFFECTS': 87, 'ASSOCIATED_WITH': 7, 'CAUSES': 3, 'Chemical': 70, 'DECREASES': 57, 'DietPattern': 42, 'Disease': 56, 'DiversityMetric': 7, 'Enzyme': 7, 'Factual': 203, 'Food': 83, 'Gene': 5, 'HAS_COMPONENT': 6, 'IMPROVES': 21, 'INCREASES': 71, 'INTERACTS_WITH': 2, 'Measurement': 31, 'Metabolite': 66, 'Microorganism': 92, 'NEG_ASSOCIATED_WITH': 15, 'Negated': 36, 'Nutrient': 120, 'POS_ASSOCIATED_WITH': 6, 'PREDISPOSES': 3, 'PREVENTS': 4, 'Physiology': 88, 'Unknown': 47, 'WORSENS': 4} \n",
      "\n",
      "{'AFFECTS': 107, 'ASSOCIATED_WITH': 28, 'CAUSES': 6, 'Chemical': 94, 'DECREASES': 101, 'DietPattern': 49, 'Disease': 102, 'DiversityMetric': 12, 'Enzyme': 16, 'Factual': 418, 'Food': 170, 'Gene': 14, 'HAS_COMPONENT': 41, 'IMPROVES': 49, 'INCREASES': 123, 'INTERACTS_WITH': 4, 'Measurement': 90, 'Metabolite': 124, 'Microorganism': 190, 'NEG_ASSOCIATED_WITH': 23, 'Negated': 44, 'Nutrient': 292, 'POS_ASSOCIATED_WITH': 32, 'PREDISPOSES': 9, 'PREVENTS': 7, 'Physiology': 221, 'Unknown': 73, 'WORSENS': 5} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_rel_dict, '\\n')\n",
    "print(dev_rel_dict, '\\n')\n",
    "print(test_rel_dict, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFFECTS(636) >> TRAIN 442(69.50%)     | DEV 87(13.68%) |     TEST 107(16.82%)\n",
      "ASSOCIATED_WITH(158) >> TRAIN 123(77.85%)     | DEV 7(4.43%) |     TEST 28(17.72%)\n",
      "CAUSES(30) >> TRAIN 21(70.00%)     | DEV 3(10.00%) |     TEST 6(20.00%)\n",
      "Chemical(507) >> TRAIN 343(67.65%)     | DEV 70(13.81%) |     TEST 94(18.54%)\n",
      "DECREASES(480) >> TRAIN 322(67.08%)     | DEV 57(11.88%) |     TEST 101(21.04%)\n",
      "DietPattern(446) >> TRAIN 355(79.60%)     | DEV 42(9.42%) |     TEST 49(10.99%)\n",
      "Disease(403) >> TRAIN 245(60.79%)     | DEV 56(13.90%) |     TEST 102(25.31%)\n",
      "DiversityMetric(61) >> TRAIN 42(68.85%)     | DEV 7(11.48%) |     TEST 12(19.67%)\n",
      "Enzyme(69) >> TRAIN 46(66.67%)     | DEV 7(10.14%) |     TEST 16(23.19%)\n",
      "Factual(2187) >> TRAIN 1566(71.60%)     | DEV 203(9.28%) |     TEST 418(19.11%)\n",
      "Food(700) >> TRAIN 447(63.86%)     | DEV 83(11.86%) |     TEST 170(24.29%)\n",
      "Gene(67) >> TRAIN 48(71.64%)     | DEV 5(7.46%) |     TEST 14(20.90%)\n",
      "HAS_COMPONENT(154) >> TRAIN 107(69.48%)     | DEV 6(3.90%) |     TEST 41(26.62%)\n",
      "IMPROVES(215) >> TRAIN 145(67.44%)     | DEV 21(9.77%) |     TEST 49(22.79%)\n",
      "INCREASES(797) >> TRAIN 603(75.66%)     | DEV 71(8.91%) |     TEST 123(15.43%)\n",
      "INTERACTS_WITH(25) >> TRAIN 19(76.00%)     | DEV 2(8.00%) |     TEST 4(16.00%)\n",
      "Measurement(379) >> TRAIN 258(68.07%)     | DEV 31(8.18%) |     TEST 90(23.75%)\n",
      "Metabolite(658) >> TRAIN 468(71.12%)     | DEV 66(10.03%) |     TEST 124(18.84%)\n",
      "Microorganism(805) >> TRAIN 523(64.97%)     | DEV 92(11.43%) |     TEST 190(23.60%)\n",
      "NEG_ASSOCIATED_WITH(97) >> TRAIN 59(60.82%)     | DEV 15(15.46%) |     TEST 23(23.71%)\n",
      "Negated(256) >> TRAIN 176(68.75%)     | DEV 36(14.06%) |     TEST 44(17.19%)\n",
      "Nutrient(1213) >> TRAIN 801(66.03%)     | DEV 120(9.89%) |     TEST 292(24.07%)\n",
      "POS_ASSOCIATED_WITH(145) >> TRAIN 107(73.79%)     | DEV 6(4.14%) |     TEST 32(22.07%)\n",
      "PREDISPOSES(30) >> TRAIN 18(60.00%)     | DEV 3(10.00%) |     TEST 9(30.00%)\n",
      "PREVENTS(39) >> TRAIN 28(71.79%)     | DEV 4(10.26%) |     TEST 7(17.95%)\n",
      "Physiology(1080) >> TRAIN 771(71.39%)     | DEV 88(8.15%) |     TEST 221(20.46%)\n",
      "Unknown(388) >> TRAIN 268(69.07%)     | DEV 47(12.11%) |     TEST 73(18.81%)\n",
      "WORSENS(25) >> TRAIN 16(64.00%)     | DEV 4(16.00%) |     TEST 5(20.00%)\n"
     ]
    }
   ],
   "source": [
    "for k in train_rel_dict:\n",
    "    total = train_rel_dict[k]+dev_rel_dict[k]+test_rel_dict[k]\n",
    "    print(f\"{k}({total}) >> TRAIN {train_rel_dict[k]}({train_rel_dict[k]*100/total:.2f}%) \\\n",
    "    | DEV {dev_rel_dict[k]}({dev_rel_dict[k]*100/total:.2f}%) | \\\n",
    "    TEST {test_rel_dict[k]}({test_rel_dict[k]*100/total:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save file names for split dataset\n",
    "# Designate your own file name\n",
    "split_filenames_dir = './split_filenames.json'\n",
    "with open(split_filenames_dir, 'w') as json_file:\n",
    "    json.dump(split_filenames, json_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py39]",
   "language": "python",
   "name": "conda-env-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
