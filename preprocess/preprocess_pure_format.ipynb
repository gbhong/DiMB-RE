{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This notebook serves as the second step: load processed files, then convert them as a task-specific input format\n",
    "- This is for Span Prediction model (PURE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from collections import defaultdict, OrderedDict\n",
    "import statistics\n",
    "\n",
    "from glob import glob\n",
    "import ssplit\n",
    "from tokenization_bert import BasicTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main part - Create two types of datasets for Span Detection Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch if you want to include all 15 types of entities\n",
    "processed_file_path = \"../data/brat/processed_files\"\n",
    "# processed_file_path = \"../data/brat/processed_files_full_entities\"\n",
    "\n",
    "# Processed files for Title+abs and Results section\n",
    "title_abs_path = os.path.join(processed_file_path, \"title_abs\")\n",
    "results_path = os.path.join(processed_file_path, \"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n",
      "165\n"
     ]
    }
   ],
   "source": [
    "# Load processed files\n",
    "ann_files = {\"Title-abs\":[], \"Results\":[]}\n",
    "txt_files = {\"Title-abs\":[], \"Results\":[]}\n",
    "\n",
    "for file_name in os.listdir(title_abs_path):\n",
    "    if file_name.endswith(\".ann\"):\n",
    "        ann_files[\"Title-abs\"].append(file_name)\n",
    "    elif file_name.endswith(\".txt\"):\n",
    "        txt_files[\"Title-abs\"].append(file_name)\n",
    "        \n",
    "for file_name in os.listdir(results_path):\n",
    "    if file_name.endswith(\".ann\"):\n",
    "        ann_files[\"Results\"].append(file_name)\n",
    "    elif file_name.endswith(\".txt\"):\n",
    "        txt_files[\"Results\"].append(file_name)\n",
    "\n",
    "for key in ann_files.keys():\n",
    "    ann_files[key].sort()\n",
    "    txt_files[key].sort()\n",
    "\n",
    "print(len(ann_files[\"Title-abs\"]))\n",
    "print(len(txt_files[\"Title-abs\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_TOKENIZER = BasicTokenizer(do_lower_case=False)\n",
    "\n",
    "def generate_sentence_boundaries(doc):\n",
    "    offsets = []\n",
    "    for start_offset, end_offset in ssplit.regex_sentence_boundary_gen(doc):\n",
    "        # Skip empty lines\n",
    "        if doc[start_offset:end_offset].strip():\n",
    "            while doc[start_offset] == \" \":\n",
    "                start_offset += 1\n",
    "            while doc[end_offset - 1] == \" \":\n",
    "                end_offset -= 1\n",
    "            assert start_offset < end_offset\n",
    "            offsets.append((start_offset, end_offset))\n",
    "    return offsets\n",
    "\n",
    "def norm_path(*paths):\n",
    "    return os.path.relpath(os.path.normpath(os.path.join(os.getcwd(), *paths)))\n",
    "\n",
    "def make_dirs(*paths):\n",
    "    os.makedirs(norm_path(*paths), exist_ok=True)\n",
    "\n",
    "def read_file(filename):\n",
    "    with open(norm_path(filename), \"rb\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def read_text(filename, encoding=None):\n",
    "    with open(norm_path(filename), \"r\", encoding=encoding) as f:\n",
    "        return f.read()\n",
    "\n",
    "def write_text(text, filename, encoding=\"UTF-8\"):\n",
    "    make_dirs(os.path.dirname(filename))\n",
    "    with open(norm_path(filename), \"w\", encoding=encoding) as f:\n",
    "        f.write(text)\n",
    "\n",
    "def read_lines(filename, encoding=None):\n",
    "    with open(norm_path(filename), \"r\", encoding=encoding) as f:\n",
    "        for line in f:\n",
    "            yield line.rstrip(\"\\r\\n\\v\")\n",
    "\n",
    "def write_lines(lines, filename, linesep=\"\\n\", encoding=\"UTF-8\"):\n",
    "    make_dirs(os.path.dirname(filename))\n",
    "    with open(norm_path(filename), \"w\", encoding=encoding) as f:\n",
    "        for line in lines:\n",
    "            f.write(line)\n",
    "            f.write(linesep)\n",
    "\n",
    "def read_json(filename, encoding=None):\n",
    "    return json.loads(read_text(filename, encoding=encoding))\n",
    "\n",
    "\n",
    "def write_json(obj, filename, indent=2, encoding=\"UTF-8\"):\n",
    "    write_text(\n",
    "        json.dumps(obj, indent=indent, ensure_ascii=False), filename, encoding=encoding\n",
    "    )\n",
    "    \n",
    "def extend_offset(offset, doc, reverse=False):\n",
    "    if reverse:\n",
    "        while offset < len(doc) and re.match(r\"[^\\W_]\", doc[offset]):\n",
    "            offset += 1\n",
    "    else:\n",
    "        while offset > 0 and re.match(r\"[^\\W_]\", doc[offset - 1]):\n",
    "            offset -= 1\n",
    "    return offset\n",
    "\n",
    "def parse_standoff_file(standoff_file, text_file, encoding=None):\n",
    "    assert os.path.exists(standoff_file), \"Standoff file not found: \" + standoff_file\n",
    "    assert os.path.exists(text_file), \"Text file not found: \" + text_file\n",
    "    \n",
    "    num_disjoint_spans = 0\n",
    "\n",
    "    entities = OrderedDict()\n",
    "    relations = OrderedDict()\n",
    "    events = OrderedDict()\n",
    "    modalities = OrderedDict()\n",
    "    attributes = OrderedDict()\n",
    "\n",
    "    # Using reference for double-check\n",
    "    reference = read_text(text_file, encoding=encoding)\n",
    "\n",
    "    for line in read_lines(standoff_file, encoding=encoding):\n",
    "        # Trim trailing whitespaces\n",
    "        line = line.strip()\n",
    "\n",
    "        if line.startswith(\n",
    "                \"T\"\n",
    "        ):  # Entities (T), Triggers (TR) (are also included in this case)\n",
    "            entity_id, entity_annotation, entity_reference = line.split(\"\\t\")\n",
    "\n",
    "            entity_id = entity_id.strip()\n",
    "            entity_annotation = entity_annotation.strip()\n",
    "            entity_reference = entity_reference.strip()\n",
    "\n",
    "            annotation_elements = entity_annotation.split(\";\")\n",
    "            entity_type, *first_offset_pair = annotation_elements[0].split()\n",
    "\n",
    "            offset_pairs = [first_offset_pair] + [\n",
    "                offset_pair.split() for offset_pair in annotation_elements[1:]\n",
    "            ]\n",
    "\n",
    "            if len(offset_pairs) > 1:\n",
    "                print(\n",
    "                    \"## Discontinuous entity found (will be excluded for this task): {} in {}\".format(\n",
    "                        entity_id, standoff_file\n",
    "                    )\n",
    "                )\n",
    "                num_disjoint_spans += 1\n",
    "                continue\n",
    "\n",
    "            start_offsets, end_offsets = list(\n",
    "                zip(\n",
    "                    *[\n",
    "                        (int(start_offset), int(end_offset))\n",
    "                        for start_offset, end_offset in offset_pairs\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "            start_offset, end_offset = min(start_offsets), max(end_offsets)\n",
    "\n",
    "            actual_reference = reference[start_offset:end_offset]\n",
    "            \n",
    "            for values in entities.values():\n",
    "                if (start_offset, end_offset) == (values['start'], values['end']):\n",
    "                    print(standoff_file)\n",
    "                    print(values)\n",
    "                    print({\n",
    "                \"id\": entity_id,\n",
    "                \"type\": entity_type,\n",
    "                \"start\": start_offset,\n",
    "                \"end\": end_offset,\n",
    "                \"ref\": actual_reference,\n",
    "            })\n",
    "            \n",
    "            entities[entity_id] = {\n",
    "                \"id\": entity_id,\n",
    "                \"type\": entity_type,\n",
    "                \"start\": start_offset,\n",
    "                \"end\": end_offset,\n",
    "                \"ref\": actual_reference,\n",
    "            }\n",
    "            \n",
    "        elif line.startswith(\"E\"):  # Relations\n",
    "            event_id, event_annotation = line.split(\"\\t\")\n",
    "\n",
    "            event_id = event_id.strip()\n",
    "            event_annotation = event_annotation.strip()\n",
    "\n",
    "            trigger, *args = event_annotation.split()\n",
    "\n",
    "            trigger_type, trigger_id = trigger.split(\":\")\n",
    "\n",
    "            args = [\n",
    "                {\"role\": arg_role, \"id\": arg_id}\n",
    "                for arg_role, arg_id in (arg.split(\":\") for arg in args)\n",
    "            ]\n",
    "            \n",
    "            for values in events.values():\n",
    "                if args == (values['args']):\n",
    "                    print(values)\n",
    "                    print({\n",
    "                \"id\": event_id,\n",
    "                \"trigger_type\": trigger_type,\n",
    "                \"trigger_id\": trigger_id,\n",
    "                \"args\": args,\n",
    "            })\n",
    "\n",
    "            events[event_id] = {\n",
    "                \"id\": event_id,\n",
    "                \"trigger_type\": trigger_type,\n",
    "                \"trigger_id\": trigger_id,\n",
    "                \"args\": args,\n",
    "            }\n",
    "            \n",
    "        elif line.startswith(\"A\"):\n",
    "            modal_id, modal_type, reference_id = line.split()\n",
    "            modalities[modal_id] = {\n",
    "                \"id\": modal_id,\n",
    "                \"type\": modal_type,\n",
    "                \"reference_ids\": reference_id,\n",
    "            }\n",
    "        elif line.startswith(\"N\"):\n",
    "            attribute_id, attribute_value = line.split(\"\\t\", 1)\n",
    "            attributes[attribute_id] = {\n",
    "                \"id\": attribute_id.strip(),\n",
    "                \"value\": attribute_value.strip(),\n",
    "            }\n",
    "        else:\n",
    "            print(\n",
    "                \"## Unexpected annotation found: {} in {}\".format(line, standoff_file)\n",
    "            )\n",
    "\n",
    "    return reference, entities, relations, events, modalities, attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "defined_types = defaultdict(set)\n",
    "\n",
    "for fn in glob(os.path.join(title_abs_path, \"**/*.txt\"), recursive=True):\n",
    "    basename, _ = os.path.splitext(fn)\n",
    "    ann_file = basename + \".ann\"\n",
    "    if os.path.exists(ann_file):\n",
    "        _, entities, _, events, _, _ = parse_standoff_file(\n",
    "            ann_file, fn, encoding=\"UTF-8\"\n",
    "        )\n",
    "        for entity in entities.values():\n",
    "            if not entity[\"type\"].isupper():\n",
    "                defined_types[\"entity_types\"].add(entity[\"type\"]) \n",
    "        for event in events.values():\n",
    "            defined_types[\"trigger_types\"].add(event[\"trigger_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(datapath):\n",
    "\n",
    "    all_documents = []\n",
    "\n",
    "    num_sents = []\n",
    "    num_tokens = []\n",
    "    num_entities = []\n",
    "    num_triggers = []\n",
    "    num_relations = []\n",
    "    num_modalities = []\n",
    "\n",
    "    inter_sent_relations = defaultdict(int)\n",
    "    modality_dict = defaultdict(int)\n",
    "\n",
    "    for fn in glob(os.path.join(datapath, \"**/*.txt\"), recursive=True):\n",
    "        print(\">> Processing: \" + fn)\n",
    "\n",
    "        doc_output = {}\n",
    "\n",
    "        sent_cnt = 0\n",
    "        tokens_cnt = 0\n",
    "\n",
    "        basename, _ = os.path.splitext(fn)\n",
    "        doc_key = basename.split(\"/\")[-1]\n",
    "        doc_output['doc_key'] = doc_key\n",
    "\n",
    "        ann_file = basename + \".ann\"\n",
    "        if os.path.exists(ann_file):\n",
    "            _, entities, _, relations, modalities, attributes = parse_standoff_file(\n",
    "                ann_file, fn, encoding=\"UTF-8\"\n",
    "            )\n",
    "        else:\n",
    "            print(\"Ann file missing: \" + ann_file)\n",
    "\n",
    "        num_relations.append(len(relations))\n",
    "        num_modalities.append(len(modalities))\n",
    "            \n",
    "        original_doc = read_text(fn, encoding=\"utf-8\")\n",
    "\n",
    "        cursor = 0\n",
    "        offset_map = {}\n",
    "        sentence_boundaries = []\n",
    "\n",
    "        # Split into sentences and ensure that there is no broken entities\n",
    "        for sentence_idx, (start_offset, end_offset) in enumerate(\n",
    "                generate_sentence_boundaries(original_doc)\n",
    "        ):\n",
    "            sentence_boundaries.append({\"start\": start_offset, \"end\": end_offset})\n",
    "            for offset in range(start_offset, end_offset + 1):\n",
    "                offset_map[offset] = {\"offset\": cursor, \"line\": sentence_idx}\n",
    "                cursor += 1  # This will include the newline at the end of sentence\n",
    "\n",
    "        # Correct broken sentence boundaries\n",
    "        for entity in entities.values():\n",
    "            entity_start, entity_end = entity[\"start\"], entity[\"end\"]\n",
    "\n",
    "            while original_doc[entity_start] == \" \":\n",
    "                entity_start += 1\n",
    "            while original_doc[entity_end - 1] == \" \":\n",
    "                entity_end -= 1\n",
    "\n",
    "            left_line_idx = offset_map[entity_start][\"line\"]\n",
    "            right_line_idx = offset_map[entity_end][\"line\"]\n",
    "            if left_line_idx != right_line_idx:\n",
    "                sentence_boundaries[min(left_line_idx, right_line_idx)][\"broken\"] = max(\n",
    "                    sentence_boundaries[min(left_line_idx, right_line_idx)].get(\n",
    "                        \"broken\", -1\n",
    "                    ),\n",
    "                    left_line_idx,\n",
    "                    right_line_idx,\n",
    "                )\n",
    "\n",
    "        # Merge broken sentences into a sentence\n",
    "        sentence_idx = 0\n",
    "        normalised_sentences = []\n",
    "        normalised_sentence_boundaries = []\n",
    "        \n",
    "        while sentence_idx < len(sentence_boundaries):\n",
    "            start_offset = sentence_boundaries[sentence_idx][\"start\"]\n",
    "            end_offset = sentence_boundaries[sentence_idx][\"end\"]\n",
    "\n",
    "            while (\n",
    "                    sentence_idx < len(sentence_boundaries)\n",
    "                    and \"broken\" in sentence_boundaries[sentence_idx]\n",
    "            ):\n",
    "                broken_sentence_idx = sentence_boundaries[sentence_idx][\"broken\"]\n",
    "                end_offset = sentence_boundaries[broken_sentence_idx][\"end\"]\n",
    "                sentence_idx = broken_sentence_idx\n",
    "\n",
    "            normalised_sentences.append(original_doc[start_offset:end_offset])\n",
    "            normalised_sentence_boundaries.append({\"start\": start_offset, \"end\": end_offset})\n",
    "            sentence_idx += 1\n",
    "            \n",
    "        sentences = []\n",
    "        doc_tokens = []\n",
    "        for sentence in normalised_sentences:\n",
    "            sent_cnt += 1\n",
    "            tokens = []\n",
    "            for token in sentence.split():\n",
    "                for subtoken in BASIC_TOKENIZER.tokenize(token):\n",
    "                    tokens_cnt += 1\n",
    "                    tokens.append(subtoken)\n",
    "            sentences.append(\" \".join(tokens))\n",
    "            doc_tokens.append(tokens)\n",
    "        \n",
    "        num_sents.append(sent_cnt)\n",
    "        num_tokens.append(tokens_cnt)\n",
    "        \n",
    "        normalized_doc = \"\\n\".join(sentences)\n",
    "        doc_output[\"sentences\"] = doc_tokens\n",
    "\n",
    "        print(\">> Building offset map...\")\n",
    "\n",
    "        # Build offset map\n",
    "        offset_map = {}\n",
    "        inverse_offset_map = {}\n",
    "\n",
    "        original_doc_pos = 0\n",
    "        normalized_doc_pos = 0\n",
    "\n",
    "        _original_doc = re.sub(r\"\\s\", \" \", original_doc) # Address special blank characters\n",
    "        _normalized_doc = normalized_doc.replace(\"\\r\", \" \").replace(\"\\n\", \" \")\n",
    "\n",
    "        while original_doc_pos < len(_original_doc) and normalized_doc_pos < len(_normalized_doc):\n",
    "            original_doc_char = _original_doc[original_doc_pos]\n",
    "            normalized_doc_char = _normalized_doc[normalized_doc_pos]\n",
    "\n",
    "            if original_doc_char == normalized_doc_char:\n",
    "                offset_map[original_doc_pos] = normalized_doc_pos\n",
    "                inverse_offset_map[normalized_doc_pos] = original_doc_pos\n",
    "                original_doc_pos += 1\n",
    "                normalized_doc_pos += 1\n",
    "            else:\n",
    "                if original_doc_char == \" \":\n",
    "                    offset_map[original_doc_pos] = normalized_doc_pos\n",
    "                    original_doc_pos += 1\n",
    "                elif normalized_doc_char == \" \":\n",
    "                    inverse_offset_map[normalized_doc_pos] = original_doc_pos\n",
    "                    normalized_doc_pos += 1                               \n",
    "                    \n",
    "        # Build Char2Token map dict\n",
    "        char2token_map = {}\n",
    "        original_doc_pos = 0\n",
    "        doc_token_idx = 0\n",
    "\n",
    "        doc_tokens_flattened = [token for sent_tokens in doc_tokens for token in sent_tokens]\n",
    "        token_cache = \"\"\n",
    "        token_reference = doc_tokens_flattened[doc_token_idx]\n",
    "        while original_doc_pos < len(_original_doc):\n",
    "            original_doc_char = _original_doc[original_doc_pos]\n",
    "            if original_doc_char == \" \":\n",
    "                original_doc_pos += 1\n",
    "                continue\n",
    "            char2token_map[original_doc_pos] = doc_token_idx\n",
    "            token_cache += original_doc_char\n",
    "            if token_cache == token_reference:\n",
    "                doc_token_idx += 1\n",
    "                if doc_token_idx == len(doc_tokens_flattened):\n",
    "                    break\n",
    "                token_reference = doc_tokens_flattened[doc_token_idx]\n",
    "                token_cache = \"\"\n",
    "            original_doc_pos += 1\n",
    "\n",
    "        if offset_map:\n",
    "            offset_map[max(offset_map) + 1] = max(offset_map.values()) + 1\n",
    "\n",
    "        if inverse_offset_map:\n",
    "            inverse_offset_map[max(inverse_offset_map) + 1] = (\n",
    "                    max(inverse_offset_map.values()) + 1\n",
    "            )\n",
    "\n",
    "        assert max(offset_map.values()) == len(_normalized_doc) \\\n",
    "        and max(inverse_offset_map) == len(_normalized_doc)  # To ensure the code above is right\n",
    "\n",
    "        doc_key = fn.split(\"/\")[-1].rstrip('.txt')\n",
    "        doc_fn = f\"./data/Reconcile_Final6/processed_files/corpora/{doc_key}\"\n",
    "\n",
    "        write_json(offset_map, doc_fn + \".map\")\n",
    "        write_text(normalized_doc, doc_fn + \".txt\")\n",
    "        write_json(char2token_map, doc_fn + \"_char2token.map\")\n",
    "        \n",
    "        sentence_boundaries_token = []\n",
    "        for span in normalised_sentence_boundaries:\n",
    "            char_start = span['start']\n",
    "            char_end = span['end']\n",
    "            while char_start not in char2token_map:\n",
    "                char_start += 1\n",
    "            while char_end not in char2token_map:\n",
    "                char_end -= 1\n",
    "                \n",
    "            token_span = {'start': char2token_map[char_start], 'end': char2token_map[char_end]+1}\n",
    "            sentence_boundaries_token.append(token_span)\n",
    "\n",
    "        # Allocate entities based on token spans\n",
    "        sent_level_entities = [[] for _ in range(len(sentence_boundaries_token))]\n",
    "        sent_level_triggers = [[] for _ in range(len(sentence_boundaries_token))]\n",
    "\n",
    "        NUM_ENT = 0\n",
    "        NUM_TRG = 0\n",
    "        for ann_id, annotation in entities.items():\n",
    "            entity_start = char2token_map[annotation['start']]\n",
    "            entity_end = char2token_map[annotation['end']-1]\n",
    "            \n",
    "            new_annotation = [entity_start, entity_end, annotation['type']]\n",
    "            \n",
    "            for sent_idx, boundary in enumerate(sentence_boundaries_token):\n",
    "                if boundary['start'] <= entity_start < boundary['end']:\n",
    "                    if not annotation['type'].isupper():\n",
    "                        NUM_ENT += 1\n",
    "                        sent_level_entities[sent_idx].append(new_annotation)\n",
    "                    else:\n",
    "                        NUM_TRG += 1\n",
    "                        sent_level_triggers[sent_idx].append(new_annotation)\n",
    "                    break\n",
    "                    \n",
    "        num_entities.append(NUM_ENT)\n",
    "        num_triggers.append(NUM_TRG)\n",
    "        \n",
    "        # Update certainty attribute for relations\n",
    "        for ann_id, annotation in modalities.items():\n",
    "            relations[annotation['reference_ids']]['modality'] = annotation['type']\n",
    "        \n",
    "        # Allocate relations based on token span of first arg\n",
    "        sent_level_relations = [[] for _ in range(len(sentence_boundaries_token))]\n",
    "        sent_level_triplets = [[] for _ in range(len(sentence_boundaries_token))]\n",
    "        \n",
    "        NUM_REL = 0\n",
    "        for ann_id, annotation in relations.items():\n",
    "            trg_type = annotation['trigger_type']\n",
    "            trg_id = annotation['trigger_id']\n",
    "            modality = annotation['modality']\n",
    "            \n",
    "            arguments = annotation['args']\n",
    "            assert len(arguments) == 2, print(annotation)\n",
    "            \n",
    "            arg1, arg2 = arguments\n",
    "            if arg2['role'] == \"Agent\":\n",
    "                arg1, arg2 = arg2, arg1\n",
    "            \n",
    "            # Exclude relations related with disjoint entities or triggers\n",
    "            if arg1['id'] not in entities \\\n",
    "            or arg2['id'] not in entities \\\n",
    "            or trg_id not in entities:\n",
    "                continue\n",
    "            \n",
    "            ent1_char_start = entities[arg1['id']]['start']\n",
    "            ent1_char_end = entities[arg1['id']]['end']\n",
    "            ent1_token_start = char2token_map[ent1_char_start]\n",
    "            ent1_token_end = char2token_map[ent1_char_end-1]\n",
    "            ent2_char_start = entities[arg2['id']]['start']\n",
    "            ent2_char_end = entities[arg2['id']]['end']\n",
    "            ent2_token_start = char2token_map[ent2_char_start]\n",
    "            ent2_token_end = char2token_map[ent2_char_end-1]\n",
    "            trg_char_start = entities[trg_id]['start']\n",
    "            trg_char_end = entities[trg_id]['end']\n",
    "            trg_token_start = char2token_map[trg_char_start]\n",
    "            trg_token_end = char2token_map[trg_char_end-1]            \n",
    "            \n",
    "            new_annotation = [\n",
    "                ent1_token_start, ent1_token_end, \n",
    "                ent2_token_start, ent2_token_end, \n",
    "                trg_type, modality\n",
    "            ]\n",
    "            new_annotation_triplet = [\n",
    "                ent1_token_start, ent1_token_end, \n",
    "                ent2_token_start, ent2_token_end, \n",
    "                trg_token_start, trg_token_end,\n",
    "                trg_type\n",
    "            ]\n",
    "            \n",
    "            new_annotation_reverse = []\n",
    "            new_annotation_triplet_reverse = []\n",
    "            if arg1['role'] != \"Agent\":\n",
    "                new_annotation_reverse = [\n",
    "                    ent2_token_start, ent2_token_end, \n",
    "                    ent1_token_start, ent1_token_end, \n",
    "                    trg_type, modality\n",
    "                ]\n",
    "                new_annotation_triplet_reverse = [\n",
    "                    ent2_token_start, ent2_token_end, \n",
    "                    ent1_token_start, ent1_token_end, \n",
    "                    trg_token_start, trg_token_end,\n",
    "                    trg_type\n",
    "                ]\n",
    "            \n",
    "            for sent_idx, boundary in enumerate(sentence_boundaries_token):\n",
    "                if boundary['start'] <= ent1_token_start < boundary['end']:\n",
    "                    sent_level_relations[sent_idx].append(new_annotation)\n",
    "                    sent_level_triplets[sent_idx].append(new_annotation_triplet)\n",
    "                    \n",
    "                    NUM_REL += 1 # don't count reversed case\n",
    "                    modality_dict[modality] += 1\n",
    "                    \n",
    "                    if new_annotation_reverse:\n",
    "                        sent_level_relations[sent_idx].append(new_annotation_reverse)\n",
    "                        sent_level_triplets[sent_idx].append(new_annotation_triplet_reverse)\n",
    "                    \n",
    "                    # Count inter-sentential relations\n",
    "                    if not boundary['start'] <= ent2_token_start < boundary['end']:\n",
    "                        inter_sent_relations[trg_type] += 1\n",
    "                        \n",
    "                    break\n",
    "                    \n",
    "        doc_output['ner'] = sent_level_entities\n",
    "        doc_output['triggers'] = sent_level_triggers\n",
    "        doc_output['relations'] = sent_level_relations\n",
    "        doc_output['triplets'] = sent_level_triplets\n",
    "        \n",
    "        doc_output['num_entities'] = NUM_ENT\n",
    "        doc_output['num_triggers'] = NUM_TRG\n",
    "        doc_output['num_relations'] = NUM_REL\n",
    "        \n",
    "        assert len(doc_output['sentences']) == len(doc_output['ner']) \\\n",
    "        == len(doc_output['triggers']) == len(doc_output['relations']), (len(doc_output['sentences']), len(doc_output['ner']) \\\n",
    "        , len(doc_output['triggers']), len(doc_output['relations']), len(normalised_sentence_boundaries))\n",
    "               \n",
    "        all_documents.append(doc_output)\n",
    "\n",
    "               \n",
    "    print(f\"\\n=== SUMMARY for {datapath.split('/')[-1]} ===\")\n",
    "    print(f\"=== NUMBER OF SENTS: {sum(num_sents)}({statistics.median(num_sents)}) ===\")\n",
    "    print(f\"=== NUMBER OF TOKENS: {sum(num_tokens)}({statistics.median(num_tokens)}) ===\")\n",
    "    print(f\"=== NUMBER OF ENTITIES: {sum(num_entities)}({statistics.median(num_entities)}) ===\")\n",
    "    print(f\"=== NUMBER OF TRIGGERS: {sum(num_triggers)}({statistics.median(num_triggers)}) ===\")\n",
    "    print(f\"=== NUMBER OF RELATIONS: {sum(num_relations)}({statistics.median(num_relations)}) ===\")\n",
    "    print(f\"=== NUMBER OF MODALITIES: {sum(num_modalities)}({statistics.median(num_modalities)}) ===\\n\\n\")\n",
    "\n",
    "    stats = {\n",
    "        \"Inter_sent_Relations\": inter_sent_relations,\n",
    "        \"Modalities\": modality_dict\n",
    "    }\n",
    "        \n",
    "    return all_documents, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "title_abs_files, title_abs_stats = preprocess(title_abs_path)\n",
    "result_files, result_stats = preprocess(results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ent_rel(ner_labels, rel_labels, all_documents):    \n",
    "    ent_cnt = {}\n",
    "    for k in ner_labels:\n",
    "        ent_cnt[k] = 0\n",
    "        \n",
    "    trg_cnt = {}\n",
    "    for k in rel_labels:\n",
    "        trg_cnt[k] = 0\n",
    "\n",
    "    rel_cnt = {}\n",
    "    for k in rel_labels:\n",
    "        rel_cnt[k] = 0\n",
    "\n",
    "    for d in all_documents:\n",
    "        for sent in d['ner']:\n",
    "            for ent in sent:\n",
    "                if len(ent) > 0:\n",
    "                    ent_cnt[ent[-1]] += 1\n",
    "        for sent in d['triggers']:\n",
    "            for trg in sent:\n",
    "                if len(trg) > 0:\n",
    "                    trg_cnt[trg[-1]] += 1\n",
    "        for sent in d['relations']:\n",
    "            for rel in sent:\n",
    "                if len(rel) > 0:\n",
    "                    rel_cnt[rel[-2]] += 1\n",
    "    \n",
    "    return ent_cnt, trg_cnt, rel_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DietPattern': 437, 'Gene': 61, 'Enzyme': 68, 'Physiology': 1028, 'Methodology': 530, 'Nutrient': 1206, 'Microorganism': 801, 'Measurement': 366, 'Disease': 403, 'Food': 685, 'Chemical': 505, 'Metabolite': 636, 'DiversityMetric': 60, 'Biospecimen': 235, 'Population': 612} | # ENTITY TOTAL: 7633\n",
      "-------------------------------------------------------------------------------\n",
      "{'DECREASES': 292, 'IMPROVES': 156, 'POS_ASSOCIATED_WITH': 81, 'NEG_ASSOCIATED_WITH': 44, 'WORSENS': 15, 'PREVENTS': 27, 'INCREASES': 402, 'AFFECTS': 276, 'INTERACTS_WITH': 18, 'CAUSES': 21, 'PREDISPOSES': 14, 'HAS_COMPONENT': 95, 'ASSOCIATED_WITH': 95} | # TRIGGER TOTAL: 1536\n",
      "-------------------------------------------------------------------------------\n",
      "{'DECREASES': 466, 'IMPROVES': 211, 'POS_ASSOCIATED_WITH': 144, 'NEG_ASSOCIATED_WITH': 108, 'WORSENS': 21, 'PREVENTS': 39, 'INCREASES': 770, 'AFFECTS': 584, 'INTERACTS_WITH': 50, 'CAUSES': 30, 'PREDISPOSES': 28, 'HAS_COMPONENT': 152, 'ASSOCIATED_WITH': 303} | # RELATION TOTAL: 2906\n"
     ]
    }
   ],
   "source": [
    "title_abs_ent_cnt, title_abs_trg_cnt, title_abs_rel_cnt = count_ent_rel(\n",
    "    defined_types['entity_types'], defined_types['trigger_types'], title_abs_files\n",
    ")\n",
    "\n",
    "print(title_abs_ent_cnt, \"| # ENTITY TOTAL:\", sum(title_abs_ent_cnt.values()))\n",
    "print(\"-\"*79)\n",
    "print(title_abs_trg_cnt, \"| # TRIGGER TOTAL:\", sum(title_abs_trg_cnt.values()))\n",
    "print(\"-\"*79)\n",
    "print(title_abs_rel_cnt, \"| # RELATION TOTAL:\", sum(title_abs_rel_cnt.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DietPattern': 398, 'Gene': 105, 'Enzyme': 167, 'Physiology': 598, 'Methodology': 412, 'Nutrient': 821, 'Microorganism': 1130, 'Measurement': 429, 'Disease': 201, 'Food': 347, 'Chemical': 608, 'Metabolite': 513, 'DiversityMetric': 166, 'Biospecimen': 70, 'Population': 568} | # ENTITY TOTAL: 6533\n",
      "-------------------------------------------------------------------------------\n",
      "{'DECREASES': 148, 'IMPROVES': 23, 'POS_ASSOCIATED_WITH': 64, 'NEG_ASSOCIATED_WITH': 39, 'WORSENS': 10, 'PREVENTS': 0, 'INCREASES': 194, 'AFFECTS': 125, 'INTERACTS_WITH': 3, 'CAUSES': 3, 'PREDISPOSES': 1, 'HAS_COMPONENT': 37, 'ASSOCIATED_WITH': 53} | # TRIGGER TOTAL: 700\n",
      "-------------------------------------------------------------------------------\n",
      "{'DECREASES': 285, 'IMPROVES': 35, 'POS_ASSOCIATED_WITH': 166, 'NEG_ASSOCIATED_WITH': 103, 'WORSENS': 10, 'PREVENTS': 0, 'INCREASES': 315, 'AFFECTS': 283, 'INTERACTS_WITH': 8, 'CAUSES': 3, 'PREDISPOSES': 1, 'HAS_COMPONENT': 64, 'ASSOCIATED_WITH': 181} | # RELATION TOTAL: 1454\n"
     ]
    }
   ],
   "source": [
    "res_ent_cnt, res_trg_cnt, res_rel_cnt = count_ent_rel(\n",
    "    defined_types['entity_types'], defined_types['trigger_types'], result_files\n",
    ")\n",
    "\n",
    "print(res_ent_cnt, \"| # ENTITY TOTAL:\", sum(res_ent_cnt.values()))\n",
    "print(\"-\"*79)\n",
    "print(res_trg_cnt, \"| # TRIGGER TOTAL:\", sum(res_trg_cnt.values()))\n",
    "print(\"-\"*79)\n",
    "print(res_rel_cnt, \"| # RELATION TOTAL:\", sum(res_rel_cnt.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_cross_sents(rel_cross_sents:dict, rel_cnt:dict):\n",
    "    result = {}\n",
    "    # Set the upper bound score based on cross-sentence relations\n",
    "    print(rel_cross_sents, \"| TOTAL:\", sum(rel_cross_sents.values()))\n",
    "    print(\"-\"*79)\n",
    "    print(\"TOTAL UPPER BOUND: \", 1 - round(sum(rel_cross_sents.values()) / sum(rel_cnt.values()), 4))\n",
    "    print(\"-\"*79)\n",
    "\n",
    "    for rel, cnt in rel_cnt.items():\n",
    "        if rel in rel_cross_sents.keys():\n",
    "            UPPER = 1 - round(rel_cross_sents[rel] / cnt, 4)\n",
    "        else: UPPER = 1.00\n",
    "        print(f\"UPPER BOUND for {rel}: {UPPER}\")\n",
    "        result[rel] = UPPER\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'AFFECTS': 114, 'DECREASES': 57, 'POS_ASSOCIATED_WITH': 5, 'INCREASES': 82, 'IMPROVES': 21, 'ASSOCIATED_WITH': 15, 'HAS_COMPONENT': 4, 'PREVENTS': 2, 'PREDISPOSES': 3, 'WORSENS': 2}) | TOTAL: 305\n",
      "-------------------------------------------------------------------------------\n",
      "TOTAL UPPER BOUND:  0.895\n",
      "-------------------------------------------------------------------------------\n",
      "UPPER BOUND for DECREASES: 0.8777\n",
      "UPPER BOUND for IMPROVES: 0.9005\n",
      "UPPER BOUND for POS_ASSOCIATED_WITH: 0.9653\n",
      "UPPER BOUND for NEG_ASSOCIATED_WITH: 1.0\n",
      "UPPER BOUND for WORSENS: 0.9048\n",
      "UPPER BOUND for PREVENTS: 0.9487\n",
      "UPPER BOUND for INCREASES: 0.8935\n",
      "UPPER BOUND for AFFECTS: 0.8048\n",
      "UPPER BOUND for INTERACTS_WITH: 1.0\n",
      "UPPER BOUND for CAUSES: 1.0\n",
      "UPPER BOUND for PREDISPOSES: 0.8929\n",
      "UPPER BOUND for HAS_COMPONENT: 0.9737\n",
      "UPPER BOUND for ASSOCIATED_WITH: 0.9505\n"
     ]
    }
   ],
   "source": [
    "abs_cross_sents_dict = count_cross_sents(title_abs_stats[\"Inter_sent_Relations\"], title_abs_rel_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'AFFECTS': 96, 'DECREASES': 38, 'INCREASES': 53, 'ASSOCIATED_WITH': 3, 'POS_ASSOCIATED_WITH': 2}) | TOTAL: 192\n",
      "-------------------------------------------------------------------------------\n",
      "TOTAL UPPER BOUND:  0.868\n",
      "-------------------------------------------------------------------------------\n",
      "UPPER BOUND for DECREASES: 0.8667\n",
      "UPPER BOUND for IMPROVES: 1.0\n",
      "UPPER BOUND for POS_ASSOCIATED_WITH: 0.988\n",
      "UPPER BOUND for NEG_ASSOCIATED_WITH: 1.0\n",
      "UPPER BOUND for WORSENS: 1.0\n",
      "UPPER BOUND for PREVENTS: 1.0\n",
      "UPPER BOUND for INCREASES: 0.8317\n",
      "UPPER BOUND for AFFECTS: 0.6608\n",
      "UPPER BOUND for INTERACTS_WITH: 1.0\n",
      "UPPER BOUND for CAUSES: 1.0\n",
      "UPPER BOUND for PREDISPOSES: 1.0\n",
      "UPPER BOUND for HAS_COMPONENT: 1.0\n",
      "UPPER BOUND for ASSOCIATED_WITH: 0.9834\n"
     ]
    }
   ],
   "source": [
    "res_cross_sents_dict = count_cross_sents(result_stats[\"Inter_sent_Relations\"], res_rel_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'Factual': 2110, 'Unknown': 364, 'Negated': 241})\n",
      "defaultdict(<class 'int'>, {'Factual': 1107, 'Negated': 196, 'Unknown': 15})\n"
     ]
    }
   ],
   "source": [
    "print(title_abs_stats['Modalities'])\n",
    "print(result_stats['Modalities'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Split only for Title-abs version\n",
    "- Load the predefined list of file names for split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_filenames_dir = './split_filenames.json'\n",
    "with open(split_filenames_dir) as json_file:\n",
    "    split_fn = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(title_abs_files, split_fn, result_files=None):\n",
    "        \n",
    "    train = []\n",
    "    train_all = []\n",
    "    dev = []\n",
    "    test = []\n",
    "    \n",
    "    for i in range(len(title_abs_files)):\n",
    "        doc_key = title_abs_files[i][\"doc_key\"]\n",
    "        if doc_key in split_fn['train']:\n",
    "            train.append(title_abs_files[i])\n",
    "        elif doc_key in split_fn['dev']:\n",
    "            dev.append(title_abs_files[i])\n",
    "        else:\n",
    "            test.append(title_abs_files[i])\n",
    "            \n",
    "    if result_files:\n",
    "        train_all = train + result_files\n",
    "#         random.shuffle(train_all)\n",
    "\n",
    "    return train, train_all, dev, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109 139 19 37\n"
     ]
    }
   ],
   "source": [
    "train_abs, train_all, dev, test = split_dataset(title_abs_files, split_fn, result_files)\n",
    "print(len(train_abs), len(train_all), len(dev), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 4283, 2082)\n",
      "(233, 652, 285)\n",
      "(494, 1321, 539)\n",
      "(3722, 9766, 3536)\n"
     ]
    }
   ],
   "source": [
    "# Detailed Stats for Train, Dev, and Test\n",
    "def dataset_stat(data):\n",
    "    sent = 0\n",
    "    ner = 0\n",
    "    rel = 0\n",
    "    for d in data:\n",
    "        for _, entity, relation in zip(d[\"sentences\"], d[\"ner\"], d[\"relations\"]):\n",
    "            sent += 1\n",
    "            ner += len(entity)\n",
    "            rel += len(relation)\n",
    "    return sent, ner, rel\n",
    "\n",
    "print(dataset_stat(train_abs))\n",
    "print(dataset_stat(dev))\n",
    "print(dataset_stat(test))\n",
    "print(dataset_stat(train_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete unnecessary keys\n",
    "for data in [train_all, dev, test]:\n",
    "    for d in data:\n",
    "        del d[\"num_entities\"]\n",
    "        del d[\"num_triggers\"]\n",
    "        del d[\"num_relations\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below is the dataset for Span Prediction Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your own path to store input files for PURE training\n",
    "outputpath = \"../data/DiMB-RE/ner_reduced_v6.1_trg_abs\"\n",
    "\n",
    "if not os.path.exists(outputpath):\n",
    "    os.mkdir(outputpath)\n",
    "\n",
    "with open(os.path.join(outputpath, \"train.json\"), 'w') as f:\n",
    "    for docu in train_abs:\n",
    "        f.write(json.dumps(docu) + '\\n')\n",
    "with open(os.path.join(outputpath, \"dev.json\"), 'w') as f:\n",
    "    for docu in dev:\n",
    "        f.write(json.dumps(docu) + '\\n')\n",
    "with open(os.path.join(outputpath, \"test.json\"), 'w') as f:\n",
    "    for docu in test:\n",
    "        f.write(json.dumps(docu) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your own path to store input files for PURE training\n",
    "outputpath = \"../data/DiMB-RE/ner_reduced_v6.1_trg_abs_result\"\n",
    "\n",
    "if not os.path.exists(outputpath):\n",
    "    os.mkdir(outputpath)\n",
    "\n",
    "with open(os.path.join(outputpath, \"train.json\"), 'w') as f:\n",
    "    for docu in train_all:\n",
    "        f.write(json.dumps(docu) + '\\n')\n",
    "with open(os.path.join(outputpath, \"dev.json\"), 'w') as f:\n",
    "    for docu in dev:\n",
    "        f.write(json.dumps(docu) + '\\n')\n",
    "with open(os.path.join(outputpath, \"test.json\"), 'w') as f:\n",
    "    for docu in test:\n",
    "        f.write(json.dumps(docu) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py39]",
   "language": "python",
   "name": "conda-env-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
